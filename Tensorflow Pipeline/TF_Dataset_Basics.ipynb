{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#### TF Dataset Basics; TF pipeline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom numpy import load\n\nimport gc\nimport cv2\n\n\n#========== Neural NW\nimport tensorflow as tf\n\nprint(\"Libraries Loaded\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Data_Loader(Batch_number):\n    path ='../input/processed-batches/Batch' + str(Batch_number) \n    dict_Batch_X = load(path + '_X.npz')\n    dict_Batch_Y = load(path + '_Y.npz')\n    Xa = dict_Batch_X['arr_0']\n    Ya = dict_Batch_Y['arr_0']\n    print(\"Batch\"+ str(Batch_number)+\":\", \"Training Data Dim\", Xa.shape,\"  Label Data Dim:\", Ya.shape)\n    del dict_Batch_X\n    del dict_Batch_Y\n    return Xa, Ya\n\n\n\n#Training_Batch = [1,2,3,4,5,6,7,8,9,10,11,12]\n#Training_Batch = [1]\nTest_Batch = 10\n\n\nX_TEST, Y_TEST =  Data_Loader(Test_Batch)\nX_TEST = X_TEST[:,:,:,0]  #single channel mode\n\n\nX, Y = Data_Loader(1)\nX = X[:,:,:,0]  #single channel mode\n\n\nprint(\"(X_TEST) Test Dataset's Dimensions:\" ,X_TEST.shape,\"label Dim: \", Y_TEST.shape)\nprint(\"(X)      Training Dataset's Dimensions:\" ,X.shape,\"label Dim: \", Y.shape)\n#print(\"Validation Dataset's Dimensions:\" ,Val.shape)\nprint()\n\n\n#Split Test images to Validation and Test\nX_Tst, X_Val, Y_Tst, Y_Val = train_test_split(X_TEST, Y_TEST, test_size=0.5, random_state=0, stratify=Y_TEST)\nprint(\"Split Test Dataset's Dimensions:\" ,X_Tst.shape,\"label Dim: \", Y_Tst.shape)\n\n#### X_Tst will stay hidden at all times\n\n#Convert to one-hot\nY_Val=tf.one_hot(Y_Val, 1000)\nY_Tst=tf.one_hot(Y_Tst, 1000)\nY=tf.one_hot(Y, 1000)\n\n\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**##Basics of TF Dataset**","metadata":{}},{"cell_type":"code","source":"\n#Converts Numpy array to a tf compatible dataset\ntf_dataset_X = tf.data.Dataset.from_tensor_slices(X)\nprint(tf_dataset_X)\nprint()\n#This is how we can iterate through dataset\nfor img in tf_dataset_X:\n    zz=img.numpy()\n    break\nprint(zz.shape)\n\n#=========================================================================================\n\n###Alternatively use this\n\n#for img in tf_dataset_X.as_numpy_iterator():\n#    print(img)\n#    break\n\n##use .take(3) to take first 3 values  and Use .skip(3) to skip first 3 values from dataset\n\n#=========================================================================================\n\n## Filter values based on something....\n\n#tf_dataset = tf_dataset.filter(lambda x: x>0)\n#for sales in tf_dataset.as_numpy_iterator():\n#    print(sales)\n\n#=========================================================================================\n### Map values based on something\n## e.g converting INR to USD with rate of 72\n\n#tf_dataset = tf_dataset.map(lambda x: x*72)\n#for sales in tf_dataset.as_numpy_iterator():\n#    print(sales)\n\n\n#=========================================================================================\n## Shuffle dataset\n\n#tf_dataset = tf_dataset.shuffle(2) #Here, 2 is the buffer value, will create group of 2 then shuffle. see ref.\n#for sales in tf_dataset.as_numpy_iterator():\n#    print(sales)\n\n\n##Ref. https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset\n#=========================================================================================\n## Batching will be used to create batches of data give value in brackets to \n## tell how many datapoints in each batch to be created\n\n#for sales_batch in tf_dataset.batch(2):\n#    print(sales_batch.numpy())\n\n\n##perform all operations in single line by combining all parameters eg.. tf_dataset.as_numpy_iterator().batch(2) etc..\n\n#=========================================================================================\n## List files in folder\nFiles_ds = tf.data.Dataset.list_files('*', shuffle=False)\n#Files_count = len(Files_ds)\nprint(\"Number of files present: \",Files_count)\nprint(\"Type:\",type(Files_ds))\nprint()\nfor file in Files_ds:\n    print(file.numpy())\n\n\n    \n#=========================================================================================\n## We can perform some basic preprocessing on images in bulk by creating a preprocessing function\n#eg:\n\n\n#def process_image(file_path):\n#    label = get_label(file_path)\n#    img = tf.io.read_file(file_path) # load the raw data from the file as a string\n#   img = tf.image.decode_jpeg(img)\n#    img = tf.image.resize(img, [128, 128])\n#    return img, label\n\n#Now apply to your datasets using .map(function_name) all Images will be processed according to the function.\n\n#train_ds = train_ds.map(process_image)\n#test_ds = test_ds.map(process_image)\n\n#eg2\n#def scale(image, label):\n#    return image/255, label\n\n#train_ds = train_ds.map(scale)\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}